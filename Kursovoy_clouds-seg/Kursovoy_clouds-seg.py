# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
import urllib.request
import tarfile
import gzip
import shutil
import matplotlib.pyplot as plt
import segmentation_models as sm

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import gc
tf.keras.backend.clear_session()
gc.collect()

# ================================
# –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–û–í–ï–†–ö–ê –î–ê–ù–ù–´–•
# ================================
def download_file(url, filename):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"""
    if not os.path.exists(filename):
        print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {filename}...")
        urllib.request.urlretrieve(url, filename)
        print(f"‚úÖ {filename} –∑–∞–≥—Ä—É–∂–µ–Ω")
    else:
        print(f"‚úÖ {filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

def extract_tar_gz(filename, extract_path):
    """–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç tar.gz –∞—Ä—Ö–∏–≤"""
    if not os.path.exists(extract_path):
        print(f"–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {filename}...")
        with tarfile.open(filename, 'r:gz') as tar:
            tar.extractall()
        print(f"‚úÖ {filename} —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω –≤ {extract_path}")
    else:
        print(f"‚úÖ {extract_path} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

def extract_gz(filename):
    """–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç gz –∞—Ä—Ö–∏–≤"""
    output_filename = filename.replace('.gz', '')
    if not os.path.exists(output_filename):
        print(f"–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {filename}...")
        with gzip.open(filename, 'rb') as f_in:
            with open(output_filename, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        print(f"‚úÖ {filename} —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω –≤ {output_filename}")
    else:
        print(f"‚úÖ {output_filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

def check_required_files():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    required_files = {
        'train.csv': 'https://video.ittensive.com/machine-learning/clouds/train.csv.gz',
        'train_images_small': 'https://video.ittensive.com/machine-learning/clouds/train_images_small.tar.gz',
        'test_images_small': 'https://video.ittensive.com/machine-learning/clouds/test_images_small.tar.gz',
        'sample_submission.csv': 'https://video.ittensive.com/machine-learning/clouds/sample_submission.csv.gz'
    }
    
    print("=" * 50)
    print("–ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("=" * 50)
    
    # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
    for file_key, url in required_files.items():
        if file_key.endswith('_images_small'):
            # –î–ª—è –∞—Ä—Ö–∏–≤–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            archive_name = f"{file_key}.tar.gz"
            download_file(url, archive_name)
            extract_tar_gz(archive_name, file_key)
        elif file_key.endswith('.csv'):
            # –î–ª—è CSV —Ñ–∞–π–ª–æ–≤
            gz_name = f"{file_key}.gz"
            download_file(url, gz_name)
            extract_gz(gz_name)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    all_exists = True
    for file_key in required_files.keys():
        if file_key.endswith('_images_small'):
            if not os.path.exists(file_key):
                print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_key}")
                all_exists = False
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å—Ç—å —Ñ–∞–π–ª—ã
                files = os.listdir(file_key)
                print(f"‚úÖ {file_key}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        else:
            if not os.path.exists(file_key):
                print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_key}")
                all_exists = False
            else:
                file_size = os.path.getsize(file_key) / (1024 * 1024)  # –≤ –ú–ë
                print(f"‚úÖ {file_key}: {file_size:.1f} –ú–±")
    
    if all_exists:
        print("üéâ –í—Å–µ —Ñ–∞–π–ª—ã –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ!")
    else:
        print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç!")
    
    return all_exists

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
if not check_required_files():
    print("‚ùå –ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.")
    exit(1)

# ================================
# –ü–ê–†–ê–ú–ï–¢–†–´
# ================================
BATCH_SIZE = 1
IMAGE_HEIGHT = 224
IMAGE_WIDTH = 224
CHANNELS = 3
TRAIN_DIR = "train_images_small"
TEST_DIR = "test_images_small"

sm.set_framework('tf.keras')

# ================================
# RLE —É—Ç–∏–ª–∏—Ç—ã
# ================================
def rle_encode(img):
    pixels = img.flatten()
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    return ' '.join(str(x) for x in runs) if len(runs) > 0 else ''

def rle_decode(mask_rle, shape=(350, 525)):
    if pd.isna(mask_rle) or mask_rle == '':
        return np.zeros(shape, dtype=np.uint8)
    s = mask_rle.split()
    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]
    starts -= 1
    ends = starts + lengths
    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)
    for lo, hi in zip(starts, ends):
        img[lo:hi] = 1
    return img.reshape(shape).T

# ================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# ================================
def load_y(df, target_shape=(IMAGE_HEIGHT, IMAGE_WIDTH)):
    masks = []
    for rle in df["EncodedPixels"]:
        mask = rle_decode(rle, shape=(350, 525))
        mask_img = tf.keras.utils.array_to_img(mask[:, :, np.newaxis])
        mask_resized = mask_img.resize((IMAGE_WIDTH, IMAGE_HEIGHT))
        mask_array = tf.keras.utils.img_to_array(mask_resized).squeeze(-1)
        masks.append((mask_array > 0.5).astype(np.float32))
    return np.array(masks)[:, :, :, np.newaxis]

def load_x(df, data_dir, target_shape=(IMAGE_HEIGHT, IMAGE_WIDTH)):
    imgs = np.empty((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS), dtype=np.float32)
    for i, fname in enumerate(df["Image"]):
        img = tf.keras.utils.load_img(
            os.path.join(data_dir, fname),
            target_size=(IMAGE_HEIGHT, IMAGE_WIDTH)
        )
        imgs[i] = tf.keras.utils.img_to_array(img)
    return imgs

# ================================
# Dice
# ================================
def dice_coef_np(y_true, y_pred, threshold=0.5, smooth=1e-6):
    y_pred_bin = (y_pred > threshold).astype(np.float32)
    intersection = np.sum(y_true * y_pred_bin)
    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred_bin) + smooth)

# ================================
# –ó–∞–≥—Ä—É–∑–∫–∞ train –¥–∞–Ω–Ω—ã—Ö
# ================================
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–æ–∫...")
data = pd.read_csv('train.csv')
data["Image"] = data["Image_Label"].str.split("_", expand=True)[0]
data["Label"] = data["Image_Label"].str.split("_", expand=True)[1]
data_fish = data[data["Label"] == "Fish"].copy()
data_fish.drop(columns=["Image_Label", "Label"], inplace=True)
del data

train_val, _ = train_test_split(data_fish, test_size=0.1, random_state=42)
train_df, val_df = train_test_split(train_val, test_size=0.2, random_state=42)
del train_val, data_fish

print(f"Train: {len(train_df)}, Val: {len(val_df)}")

X_train = load_x(train_df, TRAIN_DIR)
y_train = load_y(train_df)
X_val = load_x(val_df, TRAIN_DIR)
y_val = load_y(val_df)

# ================================
# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
# ================================
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1,
    fill_mode='nearest'
)

# ================================
# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ================================
def train_model(model_name, backbone, X_train, y_train, X_val, y_val, epochs=12):
    print(f"\n–û–±—É—á–µ–Ω–∏–µ: {model_name} + {backbone}")
    preprocess = sm.get_preprocessing(backbone)
    X_train_p = preprocess(X_train)
    X_val_p = preprocess(X_val)

    if model_name == 'FPN':
        model = sm.FPN(backbone, encoder_weights='imagenet', classes=1, activation='sigmoid')
    elif model_name == 'Unet':
        model = sm.Unet(backbone, encoder_weights='imagenet', classes=1, activation='sigmoid')
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏: FPN, Unet")

    model.compile(
        optimizer=keras.optimizers.Adam(1e-4),
        loss=sm.losses.bce_dice_loss,
        metrics=[sm.metrics.iou_score, 'binary_accuracy']
    )

    callbacks = [
        ModelCheckpoint(f"{model_name}_{backbone}_best.h5", monitor='val_loss', save_best_only=True, verbose=1),
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    ]

    model.fit(
        datagen.flow(X_train_p, y_train, batch_size=BATCH_SIZE, seed=42),
        steps_per_epoch=len(X_train) // BATCH_SIZE,
        validation_data=(X_val_p, y_val),
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    return model, preprocess

# ================================
# –û–±—É—á–∞–µ–º –¥–≤–µ –º–æ–¥–µ–ª–∏
# ================================
model1, preprocess1 = train_model('FPN', 'mobilenetv2', X_train, y_train, X_val, y_val, epochs=20)
model2, preprocess2 = train_model('Unet', 'resnet50', X_train, y_train, X_val, y_val, epochs=20)

# ================================
# –û—Ü–µ–Ω–∫–∞ –ø–æ Dice –∏ –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞
# ================================
print("\n–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
X_val_p1 = preprocess1(X_val)
X_val_p2 = preprocess2(X_val)

preds1 = model1.predict(X_val_p1, batch_size=BATCH_SIZE, verbose=0)
preds2 = model2.predict(X_val_p2, batch_size=BATCH_SIZE, verbose=0)
preds_ens = (preds1 + preds2) / 2.0

# –û—Ü–µ–Ω–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
dice1 = [dice_coef_np(y_val[i, :, :, 0], preds1[i, :, :, 0], threshold=0.4) for i in range(len(y_val))]
dice2 = [dice_coef_np(y_val[i, :, :, 0], preds2[i, :, :, 0], threshold=0.4) for i in range(len(y_val))]
print(f"\n–°—Ä–µ–¥–Ω–∏–π Dice (–ø–æ—Ä–æ–≥=0.4):")
print(f"  FPN (MobileNetV2): {np.mean(dice1):.4f}")
print(f"  Unet (ResNet50):   {np.mean(dice2):.4f}")

# –ü–æ–¥–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
best_thresh, best_dice = 0.3, 0.0
thresholds = np.arange(0.3, 0.61, 0.025)
print("\n–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞ –ø–æ Dice –∞–Ω—Å–∞–º–±–ª—è...")
for th in thresholds:
    dice_scores = [
        dice_coef_np(y_val[i, :, :, 0], preds_ens[i, :, :, 0], threshold=th)
        for i in range(len(y_val))
    ]
    avg_dice = np.mean(dice_scores)
    if avg_dice > best_dice:
        best_dice, best_thresh = avg_dice, th
    print(f"  –ü–æ—Ä–æ–≥ {th:.3f} ‚Üí Dice = {avg_dice:.4f}")

print(f"\n‚úÖ –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥: {best_thresh:.3f}, Dice = {best_dice:.4f}")
THRESHOLD = best_thresh

# ================================
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –æ—Ä–∏–≥–∏–Ω–∞–ª, –º–∞—Å–∫–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–ë–ï–ó GUI)
# ================================
print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏...")

idx = 0
orig_img = X_val[idx].astype(np.uint8)
true_mask = y_val[idx, :, :, 0].astype(np.uint8)
pred_mask = preds_ens[idx, :, :, 0]
pred_mask_binary = (pred_mask > THRESHOLD).astype(np.uint8)

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

axes[0].imshow(orig_img)
axes[0].set_title("Original Image")
axes[0].axis("off")

axes[1].imshow(orig_img)
axes[1].imshow(true_mask, cmap='Reds', alpha=0.5)
axes[1].set_title("Ground Truth Mask")
axes[1].axis("off")

axes[2].imshow(orig_img)
axes[2].imshow(pred_mask_binary, cmap='Reds', alpha=0.5)
axes[2].set_title(f"Ensemble Prediction (Threshold={THRESHOLD:.3f})")
axes[2].axis("off")

plt.tight_layout()
plt.savefig("segmentation_example.png", dpi=150, bbox_inches='tight')
print("‚úÖ –ü—Ä–∏–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: segmentation_example.png")

# –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è —Å –ª—É—á—à–∏–º –ø–æ—Ä–æ–≥–æ–º
dice_ens_final = [
    dice_coef_np(y_val[i, :, :, 0], preds_ens[i, :, :, 0], threshold=THRESHOLD)
    for i in range(len(y_val))
]
print(f"‚úÖ –ê–Ω—Å–∞–º–±–ª—å (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π Dice): {np.mean(dice_ens_final):.4f}")

# ================================
# –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ù–ê TEST –î–ê–ù–ù–´–• - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
# ================================
print("\nüìä –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ù–ê TEST –î–ê–ù–ù–´–•...")

submission = pd.read_csv('sample_submission.csv')
submission["Image"] = submission["Image_Label"].str.split("_", expand=True)[0]
submission["Label"] = submission["Image_Label"].str.split("_", expand=True)[1]

test_images = submission[submission["Label"] == "Fish"]["Image"].unique()
print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è Fish: {len(test_images)}")

predictions = []
for i, img_name in enumerate(test_images):
    if i % 50 == 0:  # –†–µ–∂–µ –≤—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(test_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    img_path = os.path.join(TEST_DIR, img_name)
    img = tf.keras.utils.load_img(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH))
    img_array = tf.keras.utils.img_to_array(img)
    img_batch = np.expand_dims(img_array, axis=0)

    pred1 = model1.predict(preprocess1(img_batch), verbose=0)[0, :, :, 0]
    pred2 = model2.predict(preprocess2(img_batch), verbose=0)[0, :, :, 0]
    pred_avg = (pred1 + pred2) / 2.0
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É (350, 525)
    pred_resized = tf.image.resize(
        pred_avg[np.newaxis, :, :, np.newaxis], 
        (350, 525), 
        method='bilinear'
    ).numpy()[0, :, :, 0]
    
    pred_bin = (pred_resized > THRESHOLD).astype(np.uint8)
    rle = rle_encode(pred_bin)
    predictions.append((img_name + "_Fish", rle))

# ================================
# –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï SUBMISSION - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
# ================================
print("\n–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...")

# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Fish
submission_fish = pd.DataFrame(predictions, columns=["Image_Label", "EncodedPixels"])

# –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º submission
final_submission = submission[["Image_Label"]].copy()
final_submission = final_submission.merge(submission_fish, on="Image_Label", how="left")
final_submission["EncodedPixels"].fillna("", inplace=True)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
fish_predictions = final_submission[final_submission["Image_Label"].str.endswith("_Fish")]
non_empty = fish_predictions[fish_predictions["EncodedPixels"] != ""]

print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ submission: {len(final_submission)}")
print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è Fish: {len(fish_predictions)}")
print(f"–ù–µ–ø—É—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Fish: {len(non_empty)}")

final_submission.to_csv("submission.csv", index=False)
print(f"\n‚úÖ Submission —Å–æ—Ö—Ä–∞–Ω—ë–Ω: submission.csv")

print("\n–ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
print(non_empty.head(10))

print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
for label in ["Fish", "Flower", "Gravel", "Sugar"]:
    label_predictions = final_submission[final_submission["Image_Label"].str.endswith(f"_{label}")]
    non_empty_count = len(label_predictions[label_predictions["EncodedPixels"] != ""])
    print(f"  {label}: {non_empty_count}/{len(label_predictions)} –Ω–µ–ø—É—Å—Ç—ã—Ö")

# ================================
# –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢
# ================================
print("\n" + "=" * 50)
print("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
print("=" * 50)

print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {THRESHOLD:.3f}")
print(f"üìà Dice –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_dice:.4f}")

print(f"\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
print("  - submission.csv - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π submission —Ñ–∞–π–ª")
print("  - segmentation_example.png - –ø—Ä–∏–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
print("  - fpn_mobilenetv2_best.h5 - –≤–µ—Å–∞ FPN –º–æ–¥–µ–ª–∏")
print("  - unet_resnet50_best.h5 - –≤–µ—Å–∞ U-Net –º–æ–¥–µ–ª–∏")

print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê SUBMISSION:")
print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(final_submission)}")
print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_images)}")
print(f"  –ù–µ–ø—É—Å—Ç—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Fish: {len(non_empty)}")

print("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
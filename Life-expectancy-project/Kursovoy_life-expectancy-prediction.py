# life_expectancy_prediction.py
"""
–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
Life Expectancy Prediction using Ensemble Methods

–ê–≤—Ç–æ—Ä: [Makarin Mikhail
Date: 2025
"""

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest
from sklearn.decomposition import PCA, FastICA, TruncatedSVD, NMF
from sklearn import manifold
import umap
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec
import os
import urllib.request
import gzip
import shutil
import tarfile
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import KFold
import re
import warnings
warnings.filterwarnings('ignore')

class LifeExpectancyPredictor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.scaler = MinMaxScaler()
        self.models = {}
        self.feature_sets = {}
        self.predictions = {}
        
    def setup_environment(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —Å—Ç–∏–ª–µ–π –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        os.makedirs('graphs', exist_ok=True)
        print("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")

    def save_plot(self, fig, filename, dpi=300):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –≤ –ø–∞–ø–∫—É graphs"""
        path = os.path.join('graphs', filename)
        fig.savefig(path, dpi=dpi, bbox_inches='tight', facecolor='white')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")

    def download_file(self, url, filename):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"""
        if not os.path.exists(filename):
            print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {filename}...")
            urllib.request.urlretrieve(url, filename)
            print(f"‚úÖ {filename} –∑–∞–≥—Ä—É–∂–µ–Ω")
        else:
            print(f"‚úÖ {filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    def check_required_files(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        required_files = {
            'rosstat.csv': 'https://video.ittensive.com/machine-learning/sc-tatar2020/rosstat/rosstat.csv'
        }
        
        print("=" * 50)
        print("–ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
        print("=" * 50)
        
        for file_key, url in required_files.items():
            self.download_file(url, file_key)
        
        all_exists = True
        for file_key in required_files.keys():
            if not os.path.exists(file_key):
                print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {file_key}")
                all_exists = False
            else:
                file_size = os.path.getsize(file_key) / (1024 * 1024)
                print(f"‚úÖ {file_key}: {file_size:.1f} –ú–±")
        
        if all_exists:
            print("üéâ –í—Å–µ —Ñ–∞–π–ª—ã –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ!")
        else:
            print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç!")
        
        return all_exists

    def linear_extrapolation(self, x):
        """–õ–∏–Ω–µ–π–Ω–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤"""
        X = np.array(x.dropna().index.astype(int)).reshape(-1, 1)
        Y = np.array(x.dropna().values).reshape(-1, 1)
        if X.shape[0] > 0:
            f = LinearRegression().fit(X, Y)
            for i in x.index:
                v = x.loc[i]
                if v != v:
                    v = f.predict([[int(i)]])[0][0]
                    if v < 0:
                        v = 0
                    x.loc[i] = v
        return x

    def clean_feature_names(self, feature_names):
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        cleaned_names = []
        for name in feature_names:
            cleaned = re.sub(r'[^\w]', '_', str(name))
            cleaned = re.sub(r'_+', '_', cleaned)
            cleaned = cleaned.strip('_')
            if not cleaned:
                cleaned = 'feature'
            cleaned_names.append(cleaned)
        return cleaned_names

    def safe_corr(self, x, y):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN"""
        mask = ~np.isnan(x) & ~np.isnan(y)
        if np.sum(mask) < 2:
            return 0
        return np.corrcoef(x[mask], y[mask])[0, 1]

    def load_and_preprocess_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("\n" + "="*50)
        print("–ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        print("="*50)

        data = pd.read_csv("rosstat.csv", na_values=["-", " - ","...","‚Ä¶"," -"])
        self.raw_data = data.copy()

        features = data["feature"]
        data.drop(labels=["feature"], inplace=True, axis=1)
        data.interpolate(method="linear", axis=1, inplace=True)
        data = data.apply(self.linear_extrapolation, axis=1, result_type="expand")
        data["feature"] = features
        data.dropna(inplace=True)
        features = data["feature"]

        data = data.T[:len(data.columns)-1].astype("float")
        data.drop(labels=["2019"], inplace=True)

        data = pd.DataFrame(self.scaler.fit_transform(data))
        data.columns = features
        
        self.features_array = np.array(features)
        self.data_scaled = data
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        self.y_column = "–û–ñ–ò–î–ê–ï–ú–ê–Ø –ü–†–û–î–û–õ–ñ–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ñ–ò–ó–ù–ò –ü–†–ò –†–û–ñ–î–ï–ù–ò–ò 1.16.1. –í—Å–µ –Ω–∞—Å–µ–ª–µ–Ω–∏–µ (—á–∏—Å–ª–æ –ª–µ—Ç)"
        self.y = data[self.y_column]
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        y_columns = [
            "–û–ñ–ò–î–ê–ï–ú–ê–Ø –ü–†–û–î–û–õ–ñ–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ñ–ò–ó–ù–ò –ü–†–ò –†–û–ñ–î–ï–ù–ò–ò 1.16.2. –ú—É–∂—á–∏–Ω—ã (—á–∏—Å–ª–æ –ª–µ—Ç)",
            "–û–ñ–ò–î–ê–ï–ú–ê–Ø –ü–†–û–î–û–õ–ñ–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ñ–ò–ó–ù–ò –ü–†–ò –†–û–ñ–î–ï–ù–ò–ò 1.16.3. –ñ–µ–Ω—â–∏–Ω—ã (—á–∏—Å–ª–æ –ª–µ—Ç)"
        ]
        
        columns_to_drop = [self.y_column]
        for col in y_columns:
            if col in data.columns:
                columns_to_drop.append(col)

        self.x = data.drop(labels=columns_to_drop, axis=1)
        
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã")
        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {self.data_scaled.shape}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {self.y_column}")
        print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {self.x.shape[1]}")

    def create_feature_sets(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ 3 –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        print("\n" + "="*50)
        print("–°–û–ó–î–ê–ù–ò–ï 3 –ù–ê–ë–û–†–û–í –ü–†–ò–ó–ù–ê–ö–û–í")
        print("="*50)

        # –ù–∞–±–æ—Ä 1: –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        print("–ù–∞–±–æ—Ä 1: –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã...")
        ensemble_matrix = self._matrix_methods_ensemble()
        self.top5_set1_indices = np.argsort(ensemble_matrix)[::-1][:5]
        self.top5_set1_features = [self.features_array[i] for i in self.top5_set1_indices]
        
        # –ù–∞–±–æ—Ä 2: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã
        print("\n–ù–∞–±–æ—Ä 2: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã...")
        ensemble_stats = self._statistical_methods_ensemble()
        self.top5_set2_indices = np.argsort(ensemble_stats)[::-1][:5]
        self.top5_set2_features = [self.features_array[i] for i in self.top5_set2_indices]
        
        # –ù–∞–±–æ—Ä 3: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
        print("\n–ù–∞–±–æ—Ä 3: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å...")
        ensemble_combined = self._combined_ensemble(ensemble_matrix, ensemble_stats)
        self.top5_set3_indices = np.argsort(ensemble_combined)[::-1][:5]
        self.top5_set3_features = [self.features_array[i] for i in self.top5_set3_indices]
        
        self._visualize_feature_sets(ensemble_matrix, ensemble_stats, ensemble_combined)

    def _matrix_methods_ensemble(self):
        """–ú–∞—Ç—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        pca = PCA(n_components=10, random_state=self.random_state).fit(self.x)
        svd = TruncatedSVD(n_components=10, random_state=self.random_state).fit(self.x)
        ica = FastICA(n_components=10, random_state=self.random_state).fit(self.x)
        nmf = NMF(n_components=10, random_state=self.random_state, max_iter=1000).fit(self.x)
        umap_model = umap.UMAP(n_components=2, random_state=self.random_state, n_jobs=1).fit(self.x)
        mds = manifold.MDS(n_components=2, random_state=self.random_state, n_init=1).fit(self.x)

        ensemble_matrix = np.zeros(len(self.x.columns))
        models_matrix = [pca, svd, ica, nmf]
        
        for model in models_matrix:
            if hasattr(model, 'components_'):
                components = np.abs(model.components_)
                for comp in components:
                    importance = MinMaxScaler().fit_transform(comp.reshape(-1, 1)).flatten()
                    importance = np.nan_to_num(importance, nan=0.0)
                    ensemble_matrix += importance

        for model in [umap_model, mds]:
            if hasattr(model, 'embedding_'):
                embedding = model.embedding_
                for comp in range(embedding.shape[1]):
                    comp_importance = np.zeros(self.x.shape[1])
                    for feat in range(self.x.shape[1]):
                        corr = self.safe_corr(embedding[:, comp], self.x.iloc[:, feat])
                        comp_importance[feat] = abs(corr)
                    comp_importance = MinMaxScaler().fit_transform(comp_importance.reshape(-1, 1)).flatten()
                    comp_importance = np.nan_to_num(comp_importance, nan=0.0)
                    ensemble_matrix += comp_importance

        return np.nan_to_num(ensemble_matrix, nan=0.0)

    def _statistical_methods_ensemble(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è
        correlations = np.array([abs(self.safe_corr(self.x[col], self.y)) for col in self.x.columns])
        correlations = MinMaxScaler().fit_transform(correlations.reshape(-1, 1)).flatten()
        correlations = np.nan_to_num(correlations, nan=0.0)

        # –í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        mi = mutual_info_regression(self.x, self.y, random_state=self.random_state)
        mi = MinMaxScaler().fit_transform(mi.reshape(-1, 1)).flatten()
        mi = np.nan_to_num(mi, nan=0.0)

        # –í–∞–∂–Ω–æ—Å—Ç—å –∏–∑ –¥–µ—Ä–µ–≤—å–µ–≤
        rf = RandomForestRegressor(n_estimators=100, random_state=self.random_state).fit(self.x, self.y)
        rf_importance = MinMaxScaler().fit_transform(rf.feature_importances_.reshape(-1, 1)).flatten()
        rf_importance = np.nan_to_num(rf_importance, nan=0.0)

        # –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
        lasso = Lasso(alpha=0.1, random_state=self.random_state, max_iter=1000).fit(self.x, self.y)
        lasso_importance = MinMaxScaler().fit_transform(np.abs(lasso.coef_).reshape(-1, 1)).flatten()
        lasso_importance = np.nan_to_num(lasso_importance, nan=0.0)

        ridge = Ridge(alpha=0.1).fit(self.x, self.y)
        ridge_importance = MinMaxScaler().fit_transform(np.abs(ridge.coef_).reshape(-1, 1)).flatten()
        ridge_importance = np.nan_to_num(ridge_importance, nan=0.0)

        return correlations + mi + rf_importance + lasso_importance + ridge_importance

    def _combined_ensemble(self, ensemble_matrix, ensemble_stats):
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å –º–µ—Ç–æ–¥–æ–≤"""
        et = ExtraTreesRegressor(n_estimators=100, random_state=self.random_state).fit(self.x, self.y)
        et_importance = MinMaxScaler().fit_transform(et.feature_importances_.reshape(-1, 1)).flatten()
        et_importance = np.nan_to_num(et_importance, nan=0.0)

        en = ElasticNet(alpha=0.1, random_state=self.random_state, max_iter=1000).fit(self.x, self.y)
        en_importance = MinMaxScaler().fit_transform(np.abs(en.coef_).reshape(-1, 1)).flatten()
        en_importance = np.nan_to_num(en_importance, nan=0.0)

        return (ensemble_matrix + ensemble_stats + et_importance + en_importance) / 4

    def _visualize_feature_sets(self, ensemble_matrix, ensemble_stats, ensemble_combined):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        fig1 = plt.figure(figsize=(15, 10))

        # –¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        plt.subplot(2, 2, 1)
        sets_features = (self.top5_set1_features + self.top5_set2_features + self.top5_set3_features)
        sets_scores = (list(ensemble_matrix[self.top5_set1_indices]) + 
                      list(ensemble_stats[self.top5_set2_indices]) + 
                      list(ensemble_combined[self.top5_set3_indices]))
        
        colors = ['blue'] * 5 + ['green'] * 5 + ['red'] * 5
        plt.barh(range(len(sets_features)), sets_scores, color=colors, alpha=0.7)
        plt.yticks(range(len(sets_features)), 
                  [f'{feat[:40]}...' if len(feat) > 40 else feat for feat in sets_features])
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
        plt.title('–¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ 3 –Ω–∞–±–æ—Ä–æ–≤')
        plt.grid(True, alpha=0.3)

        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤
        plt.subplot(2, 2, 2)
        set1_set = set(self.top5_set1_indices)
        set2_set = set(self.top5_set2_indices)
        set3_set = set(self.top5_set3_indices)

        categories = ['–¢–æ–ª—å–∫–æ –Ω–∞–±–æ—Ä 1', '–¢–æ–ª—å–∫–æ –Ω–∞–±–æ—Ä 2', '–¢–æ–ª—å–∫–æ –Ω–∞–±–æ—Ä 3', 
                     '1 –∏ 2', '1 –∏ 3', '2 –∏ 3', '–í—Å–µ —Ç—Ä–∏']
        values = [
            len(set1_set - set2_set - set3_set),
            len(set2_set - set1_set - set3_set), 
            len(set3_set - set1_set - set2_set),
            len((set1_set & set2_set) - set3_set),
            len((set1_set & set3_set) - set2_set),
            len((set2_set & set3_set) - set1_set),
            len(set1_set & set2_set & set3_set)
        ]

        plt.bar(categories, values, color=['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange'])
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.title('–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏')
        plt.grid(True, alpha=0.3)

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
        plt.subplot(2, 2, 3)
        methods = ['–ú–∞—Ç—Ä–∏—á–Ω—ã–µ', '–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ', '–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ']
        scores = [
            np.sum(ensemble_matrix[self.top5_set1_indices]),
            np.sum(ensemble_stats[self.top5_set2_indices]),
            np.sum(ensemble_combined[self.top5_set3_indices])
        ]

        plt.bar(methods, scores, color=['blue', 'green', 'red'], alpha=0.7)
        plt.ylabel('–°—É–º–º–∞—Ä–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ—Ç–±–æ—Ä–∞')
        plt.grid(True, alpha=0.3)

        for i, score in enumerate(scores):
            plt.text(i, score + max(scores)*0.01, f'{score:.2f}', ha='center', va='bottom')

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏
        plt.subplot(2, 2, 4)
        matrix_norm = ensemble_matrix / (np.max(ensemble_matrix) + 1e-10)
        stats_norm = ensemble_stats / (np.max(ensemble_stats) + 1e-10)
        combined_norm = ensemble_combined / (np.max(ensemble_combined) + 1e-10)

        plt.plot(np.sort(matrix_norm)[::-1][:20], label='–ú–∞—Ç—Ä–∏—á–Ω—ã–µ', marker='o')
        plt.plot(np.sort(stats_norm)[::-1][:20], label='–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ', marker='s')
        plt.plot(np.sort(combined_norm)[::-1][:20], label='–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ', marker='^')
        plt.xlabel('–†–∞–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–∞')
        plt.ylabel('–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        self.save_plot(fig1, '01_feature_sets_comparison.png')
        plt.show()

    def create_stacking_ensemble(self, x_train, y_train, x_test, ensemble_name):
        """–°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å —Å—Ç–µ–∫–∏–Ω–≥–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞: {ensemble_name}")
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        x_train_clean = x_train.copy()
        x_test_clean = x_test.copy()
        clean_columns = self.clean_feature_names(x_train.columns)
        x_train_clean.columns = clean_columns
        x_test_clean.columns = clean_columns
        
        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        models = {
            'RandomForest': RandomForestRegressor(n_estimators=50, random_state=self.random_state),
            'ExtraTrees': ExtraTreesRegressor(n_estimators=50, random_state=self.random_state),
            'GradientBoosting': GradientBoostingRegressor(n_estimators=50, random_state=self.random_state),
            'XGBoost': XGBRegressor(n_estimators=50, random_state=self.random_state),
            'LightGBM': LGBMRegressor(n_estimators=50, random_state=self.random_state, verbose=-1),
            'CatBoost': CatBoostRegressor(iterations=50, verbose=False, random_state=self.random_state),
            'SVR': SVR(kernel='rbf', C=1.0),
            'KNN': KNeighborsRegressor(n_neighbors=5),
            'Lasso': Lasso(alpha=0.1, random_state=self.random_state, max_iter=1000),
            'Ridge': Ridge(alpha=0.1, random_state=self.random_state),
            'ElasticNet': ElasticNet(alpha=0.1, random_state=self.random_state, max_iter=1000)
        }
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        base_predictions = []
        model_names = []
        
        for name, model in models.items():
            try:
                if name == 'LightGBM':
                    model.fit(x_train_clean, y_train)
                    pred = model.predict(x_test_clean)
                else:
                    model.fit(x_train, y_train)
                    pred = model.predict(x_test)
                
                base_predictions.append(pred)
                model_names.append(name)
                print(f"  ‚úÖ {name} –æ–±—É—á–µ–Ω–∞")
            except Exception as e:
                print(f"  ‚ùå {name} –æ—à–∏–±–∫–∞: {e}")
        
        # –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å
        if len(base_predictions) > 0:
            base_pred_matrix = np.column_stack(base_predictions)
            meta_model = LinearRegression()
            
            # Out-of-fold –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
            kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state)
            meta_features = []
            meta_target = []
            
            for train_idx, val_idx in kf.split(x_train):
                x_tr, x_val = x_train.iloc[train_idx], x_train.iloc[val_idx]
                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
                
                x_tr_clean = x_tr.copy()
                x_val_clean = x_val.copy()
                x_tr_clean.columns = self.clean_feature_names(x_tr.columns)
                x_val_clean.columns = self.clean_feature_names(x_val.columns)
                
                fold_predictions = []
                for name, model_config in models.items():
                    try:
                        if name == 'RandomForest':
                            model_clone = RandomForestRegressor(n_estimators=50, random_state=self.random_state)
                        elif name == 'ExtraTrees':
                            model_clone = ExtraTreesRegressor(n_estimators=50, random_state=self.random_state)
                        elif name == 'GradientBoosting':
                            model_clone = GradientBoostingRegressor(n_estimators=50, random_state=self.random_state)
                        elif name == 'XGBoost':
                            model_clone = XGBRegressor(n_estimators=50, random_state=self.random_state)
                        elif name == 'LightGBM':
                            model_clone = LGBMRegressor(n_estimators=50, random_state=self.random_state, verbose=-1)
                            model_clone.fit(x_tr_clean, y_tr)
                            pred = model_clone.predict(x_val_clean)
                        elif name == 'CatBoost':
                            model_clone = CatBoostRegressor(iterations=50, verbose=False, random_state=self.random_state)
                        elif name == 'SVR':
                            model_clone = SVR(kernel='rbf', C=1.0)
                        elif name == 'KNN':
                            model_clone = KNeighborsRegressor(n_neighbors=5)
                        elif name == 'Lasso':
                            model_clone = Lasso(alpha=0.1, random_state=self.random_state, max_iter=1000)
                        elif name == 'Ridge':
                            model_clone = Ridge(alpha=0.1, random_state=self.random_state)
                        elif name == 'ElasticNet':
                            model_clone = ElasticNet(alpha=0.1, random_state=self.random_state, max_iter=1000)
                        
                        if name != 'LightGBM':
                            model_clone.fit(x_tr, y_tr)
                            pred = model_clone.predict(x_val)
                        
                        fold_predictions.append(pred)
                    except Exception as e:
                        continue
                
                if fold_predictions:
                    meta_features.append(np.column_stack(fold_predictions))
                    meta_target.extend(y_val)
            
            if meta_features and meta_target:
                meta_features = np.vstack(meta_features)
                meta_model.fit(meta_features, meta_target)
                
                # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                final_prediction = meta_model.predict(base_pred_matrix)
                print(f"  üéØ –ê–Ω—Å–∞–º–±–ª—å {ensemble_name} —Å–æ–∑–¥–∞–Ω ({len(model_names)} –º–æ–¥–µ–ª–µ–π)")
                return final_prediction[0], model_names
        
        print(f"  ‚ö†Ô∏è –ê–Ω—Å–∞–º–±–ª—å {ensemble_name} –Ω–µ —Å–æ–∑–¥–∞–Ω")
        return None, []

    def prepare_training_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª–µ–π...")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.x_set1 = self.data_scaled[[self.features_array[i] for i in self.top5_set1_indices]]
        self.x_set2 = self.data_scaled[[self.features_array[i] for i in self.top5_set2_indices]]
        self.x_set3 = self.data_scaled[[self.features_array[i] for i in self.top5_set3_indices]]
        
        # –°–¥–≤–∏–≥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –≥–æ–¥)
        self.x_train_set1 = self.x_set1[:-1]
        self.y_train_set1 = self.y[1:]
        
        self.x_train_set2 = self.x_set2[:-1]
        self.y_train_set2 = self.y[1:]
        
        self.x_train_set3 = self.x_set3[:-1]
        self.y_train_set3 = self.y[1:]
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ 2019 –≥–æ–¥
        self.x_pred_set1 = self.x_set1[-1:]
        self.x_pred_set2 = self.x_set2[-1:]
        self.x_pred_set3 = self.x_set3[-1:]
        
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    def run_prediction(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print("\n" + "="*50)
        print("–°–û–ó–î–ê–ù–ò–ï 3 –ê–ù–°–ê–ú–ë–õ–ï–ô –°–¢–ï–ö–ò–ù–ì–ê")
        print("="*50)
        
        self.prepare_training_data()
        
        # –°–æ–∑–¥–∞–µ–º 3 –∞–Ω—Å–∞–º–±–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
        predictions_2019 = []
        self.ensemble_info = []
        
        # –ê–Ω—Å–∞–º–±–ª—å 1
        pred1, models1 = self.create_stacking_ensemble(
            self.x_train_set1, self.y_train_set1, self.x_pred_set1, 
            "–ê–Ω—Å–∞–º–±–ª—å 1 (–ú–∞—Ç—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã)"
        )
        if pred1 is not None:
            predictions_2019.append(pred1)
            self.ensemble_info.append(("–ê–Ω—Å–∞–º–±–ª—å 1", len(models1), models1))
        
        # –ê–Ω—Å–∞–º–±–ª—å 2
        pred2, models2 = self.create_stacking_ensemble(
            self.x_train_set2, self.y_train_set2, self.x_pred_set2, 
            "–ê–Ω—Å–∞–º–±–ª—å 2 (–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã)"
        )
        if pred2 is not None:
            predictions_2019.append(pred2)
            self.ensemble_info.append(("–ê–Ω—Å–∞–º–±–ª—å 2", len(models2), models2))
        
        # –ê–Ω—Å–∞–º–±–ª—å 3
        pred3, models3 = self.create_stacking_ensemble(
            self.x_train_set3, self.y_train_set3, self.x_pred_set3, 
            "–ê–Ω—Å–∞–º–±–ª—å 3 (–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)"
        )
        if pred3 is not None:
            predictions_2019.append(pred3)
            self.ensemble_info.append(("–ê–Ω—Å–∞–º–±–ª—å 3", len(models3), models3))
        
        if predictions_2019:
            self.final_prediction_2019 = np.mean(predictions_2019)
            self._process_results(predictions_2019)
            self._predict_2020()
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")

    def _process_results(self, predictions_2019):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "="*50)
        print("–ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–ï –ù–ê 2019 –ì–û–î")
        print("="*50)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω—É—é —à–∫–∞–ª—É
        data_temp = self.data_scaled.copy()
        data_temp[self.y_column] = np.ones(len(data_temp)) * self.final_prediction_2019
        data_original = self.scaler.inverse_transform(data_temp)
        self.predicted_life_2019 = data_original[0][np.where(self.features_array == self.y_column)[0][0]]
        
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"üéØ –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –Ω–∞ 2019 –≥–æ–¥: {self.predicted_life_2019:.2f} –ª–µ—Ç")
        print(f"üî¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∞–Ω—Å–∞–º–±–ª–µ–π: {len(predictions_2019)}")
        
        print("\nü§ñ –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û–ë –ê–ù–°–ê–ú–ë–õ–Ø–•:")
        for name, count, models in self.ensemble_info:
            print(f"  {name}: {count} –º–æ–¥–µ–ª–µ–π")
            print(f"    –ú–æ–¥–µ–ª–∏: {', '.join(models[:5])}{'...' if len(models) > 5 else ''}")
        
        self._visualize_results(predictions_2019)

    def _visualize_results(self, predictions_2019):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
        fig2 = plt.figure(figsize=(15, 10))
        
        # –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑
        plt.subplot(2, 2, 1)
        years = self.data_scaled.index.astype(str)
        actual_life = self.scaler.inverse_transform(self.data_scaled)[:, np.where(self.features_array == self.y_column)[0][0]]
        
        plt.plot(years, actual_life, marker='o', linewidth=2, label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ', color='blue')
        plt.axhline(y=self.predicted_life_2019, color='red', linestyle='--', linewidth=2, 
                   label=f'–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 2019: {self.predicted_life_2019:.2f} –ª–µ—Ç')
        plt.xlabel('–ì–æ–¥')
        plt.ylabel('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∂–∏–∑–Ω–∏ (–ª–µ—Ç)')
        plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–Ω—Å–∞–º–±–ª–µ–π
        plt.subplot(2, 2, 2)
        ensemble_names = [info[0] for info in self.ensemble_info]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∏—Å—Ö–æ–¥–Ω—É—é —à–∫–∞–ª—É
        original_preds = []
        for pred in predictions_2019:
            data_temp = self.data_scaled.copy()
            data_temp[self.y_column] = np.ones(len(data_temp)) * pred
            data_original = self.scaler.inverse_transform(data_temp)
            original_pred = data_original[0][np.where(self.features_array == self.y_column)[0][0]]
            original_preds.append(original_pred)
        
        colors = ['lightblue', 'lightgreen', 'lightcoral']
        bars = plt.bar(ensemble_names, original_preds, color=colors[:len(ensemble_names)], alpha=0.7)
        plt.ylabel('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∂–∏–∑–Ω–∏ (–ª–µ—Ç)')
        plt.title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–Ω—Å–∞–º–±–ª–µ–π')
        plt.grid(True, alpha=0.3)
        
        for bar, pred in zip(bars, original_preds):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{pred:.2f}', ha='center', va='bottom')
        
        # –°–æ—Å—Ç–∞–≤ –∞–Ω—Å–∞–º–±–ª–µ–π
        plt.subplot(2, 2, 3)
        sizes = [info[1] for info in self.ensemble_info]
        plt.pie(sizes, labels=ensemble_names, autopct='%1.1f%%', colors=colors[:len(ensemble_names)])
        plt.title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—è—Ö')
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        plt.subplot(2, 2, 4)
        feature_set_quality = []
        for i, ensemble_name in enumerate(ensemble_names):
            if i < len(original_preds):
                deviation = abs(original_preds[i] - self.predicted_life_2019)
                quality_score = 1.0 / (1.0 + deviation)
                feature_set_quality.append(quality_score)
            else:
                feature_set_quality.append(0.5)
        
        while len(feature_set_quality) < 3:
            feature_set_quality.append(0.5)
        
        set_names = ['–ù–∞–±–æ—Ä 1', '–ù–∞–±–æ—Ä 2', '–ù–∞–±–æ—Ä 3'][:len(feature_set_quality)]
        
        plt.bar(set_names, feature_set_quality, color=['blue', 'green', 'red'], alpha=0.7)
        plt.ylabel('–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
        plt.title('–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n(–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞)')
        plt.grid(True, alpha=0.3)
        
        for i, quality in enumerate(feature_set_quality):
            plt.text(i, quality + 0.01, f'{quality:.2f}', ha='center', va='bottom')
        
        plt.tight_layout()
        self.save_plot(fig2, '02_prediction_results_2019.png')
        plt.show()

    def _predict_2020(self):
        """–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 2020 –≥–æ–¥ —Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–µ–π —Ç—Ä–µ–Ω–¥–æ–≤"""
        print("\n" + "="*50)
        print("–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –ü–†–û–ì–ù–û–ó –ù–ê 2020 –ì–û–î")
        print("="*50)
        
        print("‚ö†Ô∏è  –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ 2020 –≥–æ–¥ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –∑–∞ 2019 –≥–æ–¥.")
        print("üìà –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏...")
        
        def extrapolate_trend(series, future_years=1):
            """–≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"""
            X = np.arange(len(series)).reshape(-1, 1)
            model = LinearRegression().fit(X, series)
            future_X = np.arange(len(series), len(series) + future_years).reshape(-1, 1)
            return model.predict(future_X)[0]
        
        # –ü—Ä–æ–≥–Ω–æ–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–∞ 2019 –≥–æ–¥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        x_2020_set1 = []
        for feature_idx in self.top5_set1_indices:
            feature_series = self.data_scaled[self.features_array[feature_idx]]
            predicted_value = extrapolate_trend(feature_series.values)
            x_2020_set1.append(predicted_value)
        
        x_2020_set2 = []
        for feature_idx in self.top5_set2_indices:
            feature_series = self.data_scaled[self.features_array[feature_idx]]
            predicted_value = extrapolate_trend(feature_series.values)
            x_2020_set2.append(predicted_value)
        
        x_2020_set3 = []
        for feature_idx in self.top5_set3_indices:
            feature_series = self.data_scaled[self.features_array[feature_idx]]
            predicted_value = extrapolate_trend(feature_series.values)
            x_2020_set3.append(predicted_value)
        
        # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 2020 –≥–æ–¥
        predictions_2020 = []
        
        pred1, models1 = self.create_stacking_ensemble(
            self.x_train_set1, self.y_train_set1, 
            pd.DataFrame([x_2020_set1], columns=self.x_train_set1.columns), 
            "–ê–Ω—Å–∞–º–±–ª—å 1 –¥–ª—è 2020"
        )
        if pred1 is not None:
            predictions_2020.append(pred1)
        
        pred2, models2 = self.create_stacking_ensemble(
            self.x_train_set2, self.y_train_set2, 
            pd.DataFrame([x_2020_set2], columns=self.x_train_set2.columns), 
            "–ê–Ω—Å–∞–º–±–ª—å 2 –¥–ª—è 2020"
        )
        if pred2 is not None:
            predictions_2020.append(pred2)
        
        pred3, models3 = self.create_stacking_ensemble(
            self.x_train_set3, self.y_train_set3, 
            pd.DataFrame([x_2020_set3], columns=self.x_train_set3.columns), 
            "–ê–Ω—Å–∞–º–±–ª—å 3 –¥–ª—è 2020"
        )
        if pred3 is not None:
            predictions_2020.append(pred3)
        
        if predictions_2020:
            final_prediction_2020 = np.mean(predictions_2020)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∏—Å—Ö–æ–¥–Ω—É—é —à–∫–∞–ª—É
            data_temp = self.data_scaled.copy()
            data_temp[self.y_column] = np.ones(len(data_temp)) * final_prediction_2020
            data_original = self.scaler.inverse_transform(data_temp)
            self.predicted_life_2020 = data_original[0][np.where(self.features_array == self.y_column)[0][0]]
            
            print(f"üéØ –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –Ω–∞ 2020 –≥–æ–¥: {self.predicted_life_2020:.2f} –ª–µ—Ç")
            print(f"üìà –ü—Ä–∏—Ä–æ—Å—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 2019 –≥–æ–¥–æ–º: {self.predicted_life_2020 - self.predicted_life_2019:+.2f} –ª–µ—Ç")
            
            self._visualize_2020_prediction()
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 2020 –≥–æ–¥")

    def _visualize_2020_prediction(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ 2020 –≥–æ–¥"""
        fig3 = plt.figure(figsize=(12, 6))

        years = self.data_scaled.index.astype(str)
        actual_life = self.scaler.inverse_transform(self.data_scaled)[:, np.where(self.features_array == self.y_column)[0][0]]
        
        years_extended = list(years) + ['2019', '2020']
        life_extended = list(actual_life) + [self.predicted_life_2019, self.predicted_life_2020]

        plt.plot(years_extended[:-2], life_extended[:-2], marker='o', linewidth=2,
                label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ', color='blue')
        plt.plot(['2018', '2019', '2020'], [actual_life[-1], self.predicted_life_2019, self.predicted_life_2020],
                marker='s', linewidth=2, label='–ü—Ä–æ–≥–Ω–æ–∑', color='red', linestyle='--')
        plt.xlabel('–ì–æ–¥')
        plt.ylabel('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∂–∏–∑–Ω–∏ (–ª–µ—Ç)')
        plt.title('–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –Ω–∞ 2019-2020 –≥–æ–¥—ã')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

        plt.tight_layout()
        self.save_plot(fig3, '03_2020_prediction.png')
        plt.show()

    def run_complete_analysis(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ü–†–û–î–û–õ–ñ–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ñ–ò–ó–ù–ò")
        print("="*60)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.setup_environment()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if not self.check_required_files():
            print("‚ùå –ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.")
            return
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self.load_and_preprocess_data()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.create_feature_sets()
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.run_prediction()
        
        print("\n" + "="*50)
        print("–í–´–ü–û–õ–ù–ï–ù–ò–ï –ó–ê–î–ê–ù–ò–Ø –ó–ê–í–ï–†–®–ï–ù–û")
        print("="*50)
        print("‚úÖ –°–æ–∑–¥–∞–Ω–æ 3 –Ω–∞–±–æ—Ä–∞ –ø–æ 5 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print("‚úÖ –°–æ–∑–¥–∞–Ω–æ 3 –∞–Ω—Å–∞–º–±–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞ —Å —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏")
        print("‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –Ω–∞ 2019 –≥–æ–¥")
        print("‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω –ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –Ω–∞ 2020 –≥–æ–¥ (—Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–µ–π)")
        print("üìä –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'graphs'")
        print("="*50)


# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
if __name__ == "__main__":
    predictor = LifeExpectancyPredictor(random_state=42)
    predictor.run_complete_analysis()
# Классификация и ансамбли

## Описание проекта
Этот проект решает задачу классификации. Цель - предсказать уровень риска (Response от 1 до 8) для страховых клиентов на основе их медицинских и демографических данных.

## Стек технологий
- **Python 3.x**
- **Библиотеки машинного обучения**:
  - pandas, numpy
  - scikit-learn
  - CatBoost
  - XGBoost
  - LightGBM
- **Ансамблевые методы**: Стекинг (Stacking)

## Структура проекта
project/
|── zadaniye_klassifikatsiya_i_ansambli.py # Основной скрипт
|── submission_stacking.csv # Файл с предсказаниями
|── README.md # Документация

## Методология

### Предобработка данных
- Извлечение признаков из категориальных переменных (Product_Info_2)
- Заполнение пропусков значением -1
- Нормализация данных с помощью StandardScaler
- Оптимизация использования памяти

### Модели машинного обучения
В проекте используются и сравниваются 4 алгоритма градиентного бустинга:
1. **XGBoost**
2. **CatBoost** 
3. **GradientBoosting** (scikit-learn)
4. **LightGBM**

### Ансамблевый подход
Создан **стекинг-ансамбль**, который объединяет предсказания всех 4 моделей с помощью логистической регрессии в качестве мета-классификатора.

### Метрика оценки
Основная метрика - **Quadratic Kappa** (коэффициент согласованности Коэна с квадратичными весами), которая является официальной метрикой соревнования.

## Результаты

### Лучшие гиперпараметры моделей
| Модель               | Лучшие параметры                                              |
|----------------------|---------------------------------------------------------------|
| **XGBoost**          | `{'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200}` |
| **CatBoost**         | `{'depth': 4, 'iterations': 1000, 'learning_rate': 0.1}`      |
| **GradientBoosting** | `{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}` |
| **LightGBM**         | `{'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200}` |

### Сравнение качества моделей
Модели показали следующие результаты на валидационной выборке:
| Модель               | Точность   | Quadratic Kappa |
|----------------------|------------|-----------------|
| XGBoost              | 0.5904     | 0.5451          |
| CatBoost             | 0.5566     | 0.5534          |
| GradientBoosting     | 0.5824     | 0.5462          |
| LightGBM             | 0.5898     | 0.5512          |
| **Стекинг-ансамбль** | **0.7249** | **0.7025**      |

### Финальные результаты на полных данных
- **Точность стекинг-ансамбля**: 0.7276
- **Quadratic Kappa стекинг-ансамбля**: 0.7129

## Выводы
- **Стекинг-ансамбль** значительно превосходит индивидуальные модели по всем метрикам
- Улучшение Kappa score: **+15-16%** по сравнению с лучшей индивидуальной моделью
- Улучшение точности: **+13-14%** по сравнению с лучшей индивидуальной моделью
- Наибольший вклад в ансамбль вносят XGBoost и LightGBM

## Запуск проекта
1. Установите необходимые зависимости:
bash
pip install pandas numpy scikit-learn catboost xgboost lightgbm
2. Запустите основной скрипт:
bash
python zadaniye_klassifikatsiya_i_ansambli.py

## Особенности реализации
- Автоматический подбор гиперпараметров через GridSearchCV
- Стратифицированное разделение данных 
- Оптимизация использования памяти для работы с большими данными
- Параллельные вычисления (n_jobs=-1)
- Детальное логирование процесса обучения

## Данные
Данные загружаются автоматически по URL:
- Обучающая выборка: train.csv.gz
- Тестовая выборка: test.csv.gz
- Пример submission: sample_submission.csv.gz

## Автор
Проект выполнен в рамках учебного курса по машинному обучению.

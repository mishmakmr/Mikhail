# -*- coding: utf-8 -*-
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

import gymnasium as gym
import numpy as np
import random
import collections
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from collections import deque

print("=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø DQN –î–õ–Ø MOUNTAINCAR-V0 ===")

# –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, output_size)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.xavier_uniform_(self.fc3.weight)
        nn.init.xavier_uniform_(self.fc4.weight)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=100000)
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.9995
        self.learning_rate = 0.00025
        self.batch_size = 128
        self.update_target_every = 100
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        self.model = DQN(state_size, action_size).to(self.device)
        self.target_model = DQN(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-5)
        self.update_target_network()
        self.train_step = 0
        
    def update_target_network(self):
        self.target_model.load_state_dict(self.model.state_dict())
        
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state, training=True):
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.model(state)
        return np.argmax(q_values.cpu().data.numpy())
    
    def replay(self):
        if len(self.memory) < self.batch_size:
            return
            
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(np.array(actions)).to(self.device)
        rewards = torch.FloatTensor(np.array(rewards)).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.BoolTensor(np.array(dones)).to(self.device)
        
        # Double DQN
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))
        next_actions = self.model(next_states).max(1)[1].unsqueeze(1)
        next_q_values = self.target_model(next_states).gather(1, next_actions).squeeze()
        
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)  # Huber loss
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.0)
        self.optimizer.step()
        
        self.train_step += 1
        if self.train_step % self.update_target_every == 0:
            self.update_target_network()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

def train_mountain_car_final():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π"""
    try:
        env = gym.make('MountainCar-v0')
        state_size = env.observation_space.shape[0]
        action_size = env.action_space.n
        
        print(f"‚úì –°—Ä–µ–¥–∞: MountainCar-v0")
        print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: {state_size} (–ø–æ–∑–∏—Ü–∏—è, —Å–∫–æ—Ä–æ—Å—Ç—å)")
        print(f"–î–µ–π—Å—Ç–≤–∏—è: {action_size} (0-–≤–ª–µ–≤–æ, 1-—Å—Ç–æ–ø, 2-–≤–ø—Ä–∞–≤–æ)")
        print(f"–¶–µ–ª—å: –¥–æ—Å—Ç–∏—á—å –ø–æ–∑–∏—Ü–∏–∏ ‚â• 0.5")
        
        agent = DQNAgent(state_size, action_size)
        episodes = 400000
        scores = []
        steps_history = []
        max_positions = []
        success_history = []
        
        print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        print("–≠–ø–∏–∑–æ–¥\t–®–∞–≥–∏\t–ú–∞–∫—Å.–ü–æ–∑\t–£—Å–ø–µ—Ö–∏\tEpsilon")
        print("-" * 55)
        
        for episode in range(episodes):
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            max_position = -1.2
            positions = []
            
            while True:
                action = agent.act(state)
                next_state, reward, done, truncated, _ = env.step(action)
                
                position = next_state[0]
                velocity = next_state[1]
                max_position = max(max_position, position)
                positions.append(position)
                
                # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ù–ê–ì–†–ê–î
                if position >= 0.5:
                    reward = 100.0  # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —É—Å–ø–µ—Ö
                    done = True
                else:
                    # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å
                    reward = position * 10  # –ß–µ–º –≤—ã—à–µ –ø–æ–∑–∏—Ü–∏—è, —Ç–µ–º –ª—É—á—à–µ
                    
                    # –ë–æ–Ω—É—Å –∑–∞ —Å–∫–æ—Ä–æ—Å—Ç—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏
                    if (position < -0.2 and velocity < 0) or (position > -0.2 and velocity > 0):
                        reward += abs(velocity) * 5
                    
                    # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—Ä–µ–º—è
                    reward -= 0.1
                
                agent.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                steps += 1
                
                if done or steps >= 1000:
                    break
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–∞—Ç—á–∞—Ö
            if len(agent.memory) > agent.batch_size:
                for _ in range(4):
                    agent.replay()
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            success = max_position >= 0.5
            scores.append(total_reward)
            steps_history.append(steps)
            max_positions.append(max_position)
            success_history.append(1 if success else 0)
            
            # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if episode % 100 == 0 or success:
                recent_success = np.mean(success_history[-100:]) * 100 if len(success_history) >= 100 else 0
                recent_avg_pos = np.mean(max_positions[-100:]) if len(max_positions) >= 100 else max_position
                status = " üéâ" if success else ""
                print(f"{episode}\t{steps}\t{max_position:.3f}\t{recent_success:.1f}%\t{agent.epsilon:.3f}{status}")
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º —É—Å–ø–µ—Ö–µ
            if len(success_history) >= 100:
                recent_success_rate = np.mean(success_history[-100:]) * 100
                if recent_success_rate >= 90:
                    print(f"\nüéâ –î–û–°–¢–ò–ì–ù–£–¢–ê –¶–ï–õ–¨ –Ω–∞ —ç–ø–∏–∑–æ–¥–µ {episode}!")
                    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {recent_success_rate:.1f}%")
                    break
        
        env.close()
        torch.save(agent.model.state_dict(), 'mountain_car_final.pth')
        print("‚úì –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
        return agent, scores, steps_history, max_positions, success_history
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return None, None, None, None, None

def analyze_final_results(scores, steps_history, max_positions, success_history):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    total_episodes = len(scores)
    success_count = sum(success_history)
    
    print(f"\n{'='*60}")
    print("üìä –§–ò–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print(f"{'='*60}")
    print(f"–í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_episodes}")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤: {success_count}")
    print(f"–û–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_count/total_episodes*100:.1f}%")
    print(f"–õ—É—á—à–∞—è –ø–æ–∑–∏—Ü–∏—è: {np.max(max_positions):.3f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –ø–æ–∑–∏—Ü–∏—è: {np.mean(max_positions):.3f}")
    print(f"–°—Ä–µ–¥–Ω–∏–µ —à–∞–≥–∏: {np.mean(steps_history):.1f}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ñ–∞–∑–∞–º –æ–±—É—á–µ–Ω–∏—è
    if total_episodes >= 300:
        quarter = total_episodes // 4
        phases = [
            ("1-—è —á–µ—Ç–≤–µ—Ä—Ç—å", 0, quarter),
            ("2-—è —á–µ—Ç–≤–µ—Ä—Ç—å", quarter, quarter*2),
            ("3-—è —á–µ—Ç–≤–µ—Ä—Ç—å", quarter*2, quarter*3),
            ("4-—è —á–µ—Ç–≤–µ—Ä—Ç—å", quarter*3, total_episodes)
        ]
        
        print(f"\nüìà –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –ü–û –§–ê–ó–ê–ú:")
        for phase_name, start, end in phases:
            if start < total_episodes:
                phase_success = np.mean(success_history[start:end]) * 100
                phase_avg_pos = np.mean(max_positions[start:end])
                phase_avg_steps = np.mean(steps_history[start:end])
                print(f"{phase_name}: {phase_success:.1f}% —É—Å–ø–µ—Ö–∞, –ø–æ–∑–∏—Ü–∏—è {phase_avg_pos:.3f}, —à–∞–≥–∏ {phase_avg_steps:.1f}")

def create_comprehensive_plots(scores, steps_history, max_positions, success_history):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
    plt.figure(figsize=(18, 12))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
    plt.subplot(2, 3, 1)
    x = range(len(max_positions))
    plt.scatter(x, max_positions, alpha=0.3, s=1, color='blue', label='–ú–∞–∫—Å. –ø–æ–∑–∏—Ü–∏—è')
    
    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    if len(max_positions) >= 50:
        window = 50
        moving_avg = [np.mean(max_positions[i:i+window]) for i in range(len(max_positions)-window+1)]
        plt.plot(range(window-1, len(max_positions)), moving_avg, 'red', linewidth=2, label='–°—Ä–µ–¥–Ω–µ–µ –∑–∞ 50 —ç–ø.')
    
    plt.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='–¶–µ–ª—å')
    plt.xlabel('–≠–ø–∏–∑–æ–¥')
    plt.ylabel('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è')
    plt.title('–ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å
    plt.subplot(2, 3, 2)
    if len(success_history) >= 100:
        window = 100
        success_rates = [np.mean(success_history[i:i+window]) * 100 for i in range(len(success_history)-window+1)]
        plt.plot(range(window-1, len(success_history)), success_rates, 'purple', linewidth=2)
        plt.axhline(y=90, color='orange', linestyle='--', label='–¶–µ–ª—å 90%')
        plt.xlabel('–≠–ø–∏–∑–æ–¥')
        plt.ylabel('–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)')
        plt.title('–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
    plt.subplot(2, 3, 3)
    successful_positions = [pos for pos, success in zip(max_positions, success_history) if success]
    unsuccessful_positions = [pos for pos, success in zip(max_positions, success_history) if not success]
    
    plt.hist(successful_positions, bins=20, alpha=0.7, color='green', label='–£—Å–ø–µ—à–Ω—ã–µ', edgecolor='black')
    plt.hist(unsuccessful_positions, bins=20, alpha=0.7, color='red', label='–ù–µ—É—Å–ø–µ—à–Ω—ã–µ', edgecolor='black')
    plt.axvline(x=0.5, color='blue', linestyle='--', linewidth=2, label='–¶–µ–ª—å')
    plt.xlabel('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –®–∞–≥–∏ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º
    plt.subplot(2, 3, 4)
    successful_steps = [steps for steps, success in zip(steps_history, success_history) if success]
    unsuccessful_steps = [steps for steps, success in zip(steps_history, success_history) if not success]
    
    plt.scatter(range(len(successful_steps)), successful_steps, alpha=0.6, color='green', s=10, label='–£—Å–ø–µ—à–Ω—ã–µ')
    plt.scatter(range(len(unsuccessful_steps)), unsuccessful_steps, alpha=0.3, color='red', s=10, label='–ù–µ—É—Å–ø–µ—à–Ω—ã–µ')
    plt.xlabel('–≠–ø–∏–∑–æ–¥')
    plt.ylabel('–®–∞–≥–∏')
    plt.title('–®–∞–≥–∏ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 5: –ù–∞–≥—Ä–∞–¥—ã
    plt.subplot(2, 3, 5)
    successful_scores = [score for score, success in zip(scores, success_history) if success]
    unsuccessful_scores = [score for score, success in zip(scores, success_history) if not success]
    
    plt.scatter(range(len(successful_scores)), successful_scores, alpha=0.6, color='green', s=10, label='–£—Å–ø–µ—à–Ω—ã–µ')
    plt.scatter(range(len(unsuccessful_scores)), unsuccessful_scores, alpha=0.3, color='red', s=10, label='–ù–µ—É—Å–ø–µ—à–Ω—ã–µ')
    plt.xlabel('–≠–ø–∏–∑–æ–¥')
    plt.ylabel('–ù–∞–≥—Ä–∞–¥–∞')
    plt.title('–ù–∞–≥—Ä–∞–¥—ã –ø–æ —ç–ø–∏–∑–æ–¥–∞–º')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 6: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —ç–ø–∏–∑–æ–¥–æ–≤)
    plt.subplot(2, 3, 6)
    recent_episodes = min(200, len(max_positions))
    recent_positions = max_positions[-recent_episodes:]
    recent_success = success_history[-recent_episodes:]
    
    colors = ['green' if success else 'red' for success in recent_success]
    plt.scatter(range(recent_episodes), recent_positions, c=colors, alpha=0.6, s=20)
    plt.axhline(y=0.5, color='blue', linestyle='--', linewidth=2, label='–¶–µ–ª—å')
    plt.xlabel('–≠–ø–∏–∑–æ–¥ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ)')
    plt.ylabel('–ú–∞–∫—Å. –ø–æ–∑–∏—Ü–∏—è')
    plt.title('–§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('mountain_car_final_analysis.png', dpi=150, bbox_inches='tight')
    print("‚úì –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

def run_final_evaluation(agent, num_episodes=50):
    """–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞"""
    print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ({num_episodes} —ç–ø–∏–∑–æ–¥–æ–≤)")
    
    env = gym.make('MountainCar-v0')
    results = []
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        steps = 0
        positions = []
        
        while True:
            action = agent.act(state, training=False)
            next_state, reward, done, truncated, _ = env.step(action)
            
            positions.append(next_state[0])
            state = next_state
            steps += 1
            
            if done or steps >= 1000:
                max_pos = max(positions)
                success = max_pos >= 0.5
                results.append({
                    'steps': steps,
                    'max_position': max_pos,
                    'success': success
                })
                break
    
    env.close()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    success_rate = np.mean([r['success'] for r in results]) * 100
    avg_steps = np.mean([r['steps'] for r in results])
    avg_position = np.mean([r['max_position'] for r in results])
    
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
    print(f"–°—Ä–µ–¥–Ω–∏–µ —à–∞–≥–∏: {avg_steps:.1f}")
    print(f"–°—Ä–µ–¥–Ω—è—è –ø–æ–∑–∏—Ü–∏—è: {avg_position:.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    if success_rate >= 90:
        print("üéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –ê–≥–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É")
    elif success_rate >= 70:
        print("‚úÖ –•–û–†–û–®–û! –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É")
    elif success_rate >= 50:
        print("‚ö†Ô∏è –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û! –ê–≥–µ–Ω—Ç —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É, –Ω–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ")
    else:
        print("‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–û–ë–£–ß–ï–ù–ò–ï!")
    
    return results

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    agent, scores, steps_history, max_positions, success_history = train_mountain_car_final()
    
    if agent and scores:
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analyze_final_results(scores, steps_history, max_positions, success_history)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        create_comprehensive_plots(scores, steps_history, max_positions, success_history)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_results = run_final_evaluation(agent, 50)
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥
        final_success_rate = np.mean([r['success'] for r in test_results]) * 100
        print(f"\n{'='*60}")
        print("üéì –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        print(f"{'='*60}")
        print(f"–ê–ª–≥–æ—Ä–∏—Ç–º: Deep Q-Network (DQN) —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏")
        print(f"–°—Ä–µ–¥–∞: MountainCar-v0")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {final_success_rate:.1f}% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏")
        
        if final_success_rate >= 70:
            print("‚úÖ –ó–ê–î–ê–ß–ê –£–°–ü–ï–®–ù–û –†–ï–®–ï–ù–ê!")
            print("–ê–≥–µ–Ω—Ç –æ—Å–≤–æ–∏–ª —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–∫–∞—á–∫–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏")
        else:
            print("‚ö†Ô∏è –ó–∞–¥–∞—á–∞ —Ä–µ—à–µ–Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ")
            print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    
    else:
        print("‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")

print("\nüèÅ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê")